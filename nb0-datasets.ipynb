{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywsra\n",
    "import xarray as xr\n",
    "from littlebuoybigwaves import geo\n",
    "\n",
    "from configure import get_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the configuration file, `config.toml`, which contains the data directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WSRA and P-3 met data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSRA_DIR = config['dirs']['wsra']\n",
    "\n",
    "def construct_wsra_path(storm_name: str) -> str:\n",
    "    return os.path.join(WSRA_DIR, storm_name)\n",
    "\n",
    "def read_wsra_and_met_data(\n",
    "    storm_name: str,\n",
    "    met_data_vars:  str | List[str] = 'all',\n",
    "    met_rename_dict: dict[str, str] | None = None,\n",
    ") -> xr.Dataset:\n",
    "    # Open WSRA and P-3 met data.\n",
    "    directory = construct_wsra_path(storm_name)\n",
    "    wsra_ds = pywsra.read_wsra_directory(directory, index_by_time=True)\n",
    "    met_ds = pywsra.read_met_directory(os.path.join(directory, 'met'),  # TODO: use AC data?\n",
    "                                       data_vars=met_data_vars)\n",
    "\n",
    "    # Merge the datasets by resampling the P-3 met data onto the WSRA times.\n",
    "    wsra_merged_ds = pywsra.merge_met_vars(wsra_ds=wsra_ds,\n",
    "                                           met_ds=met_ds,\n",
    "                                           data_vars=met_data_vars,\n",
    "                                           resample_method=np.nanmedian,\n",
    "                                           rename_dict=met_rename_dict)\n",
    "\n",
    "    # Save `storm_name` as an attribute for future reference.\n",
    "    wsra_merged_ds.attrs['storm_name'] = storm_name.lower()\n",
    "\n",
    "    return wsra_merged_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_kwargs = dict(\n",
    "    met_data_vars = [\n",
    "        'SfmrWS.1', 'SfmrWErr.1', 'SfmrRainRate.1', 'SfmrDV.1', 'LonGPS.1', 'LatGPS.1',\n",
    "    ],\n",
    "    met_rename_dict = {\n",
    "        'SfmrWS.1': 'met_sfmr_10m_wind_speed',\n",
    "        'SfmrWErr.1': 'met_sfmr_10m_wind_speed_error',\n",
    "        'SfmrRainRate.1': 'met_sfmr_rain_rate',\n",
    "        'SfmrDV.1': 'met_sfmr_data_validity',\n",
    "        'LonGPS.1': 'met_longitude',\n",
    "        'LatGPS.1': 'met_latitude',\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacob/Programs/pywsra/src/pywsra/met.py:167: RuntimeWarning: All-NaN slice encountered\n",
      "  values.append(resample_method(met_in_window[var].values))\n",
      "/Users/jacob/Programs/pywsra/src/pywsra/met.py:167: RuntimeWarning: All-NaN slice encountered\n",
      "  values.append(resample_method(met_in_window[var].values))\n",
      "/Users/jacob/Programs/pywsra/src/pywsra/met.py:167: RuntimeWarning: All-NaN slice encountered\n",
      "  values.append(resample_method(met_in_window[var].values))\n",
      "/Users/jacob/Programs/pywsra/src/pywsra/met.py:167: RuntimeWarning: All-NaN slice encountered\n",
      "  values.append(resample_method(met_in_window[var].values))\n",
      "/Users/jacob/Programs/pywsra/src/pywsra/met.py:167: RuntimeWarning: All-NaN slice encountered\n",
      "  values.append(resample_method(met_in_window[var].values))\n",
      "/Users/jacob/Programs/pywsra/src/pywsra/met.py:167: RuntimeWarning: All-NaN slice encountered\n",
      "  values.append(resample_method(met_in_window[var].values))\n",
      "/Users/jacob/Programs/pywsra/src/pywsra/met.py:167: RuntimeWarning: All-NaN slice encountered\n",
      "  values.append(resample_method(met_in_window[var].values))\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/wsra-mss/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/Users/jacob/Programs/pywsra/src/pywsra/met.py:167: RuntimeWarning: All-NaN slice encountered\n",
      "  values.append(resample_method(met_in_window[var].values))\n"
     ]
    }
   ],
   "source": [
    "earl_ds = read_wsra_and_met_data('Earl', **read_kwargs)\n",
    "fiona_ds = read_wsra_and_met_data('Fiona', **read_kwargs)\n",
    "ian_ds = read_wsra_and_met_data('Ian', **read_kwargs)\n",
    "julia_ds = read_wsra_and_met_data('Julia', **read_kwargs)  #TODO: wrong data on Prosensing site for Julia \n",
    "franklin_ds = read_wsra_and_met_data('Franklin', **read_kwargs)\n",
    "idalia_ds = read_wsra_and_met_data('Idalia', **read_kwargs)\n",
    "lee_ds = read_wsra_and_met_data('Lee', **read_kwargs)\n",
    "atomic_ds = read_wsra_and_met_data('atomic', **read_kwargs)\n",
    "#TODO: include nigel and spotters\n",
    "# pywsra.read_wsra_file(EUREC4A_ATOMIC_P3_WSRA_20200117_20200211_with_SWIFT.nc', index_by_time=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buoys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hurricane drifter datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_drifter_data(\n",
    "    drifter_file: str,\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    with open(drifter_file, 'rb') as handle:\n",
    "        drifter_data = pickle.load(handle)\n",
    "\n",
    "    # if any(key in drifter_data for key in ['spotter', 'microswift', 'dwsd']):\n",
    "    drifter_data = merge_drifter_dicts(drifter_data)\n",
    "\n",
    "    return concatenate_drifters(drifter_data)\n",
    "\n",
    "def merge_drifter_dicts(drifter_data: dict) -> dict:\n",
    "    drifter_dict = (drifter_data.get('spotter', {}) |\n",
    "                    drifter_data.get('microswift', {}) |\n",
    "                    drifter_data.get('dwsd', {}))\n",
    "    return drifter_dict\n",
    "\n",
    "def concatenate_drifters(drifter_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatenate a dictionary of individual drifter DataFrames into a single,\n",
    "    multi-index DataFrame.  Drop the observations that do not contain waves\n",
    "    (remove off-hour pressure and temperature observations).\n",
    "\n",
    "    Args:\n",
    "        drifter_dict (dict): individual drifter DataFrames keyed by id.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: concatenated drifters\n",
    "    \"\"\"\n",
    "    drifter_df = (\n",
    "        pd.concat(drifter_dict, names=['id', 'time'])\n",
    "        .dropna(subset='energy_density')\n",
    "    )\n",
    "    return drifter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARL_DRIFTER_FILE = config['files']['earl_drifters']\n",
    "FIONA_DRIFTER_FILE = config['files']['fiona_drifters']\n",
    "IAN_DRIFTER_FILE = config['files']['ian_drifters']\n",
    "IDALIA_DRIFTER_FILE = config['files']['idalia_drifters']\n",
    "LEE_DRIFTER_FILE = config['files']['lee_drifters']\n",
    "\n",
    "earl_drifter_df = read_drifter_data(EARL_DRIFTER_FILE)\n",
    "fiona_drifter_df = read_drifter_data(FIONA_DRIFTER_FILE)\n",
    "ian_drifter_df = read_drifter_data(IAN_DRIFTER_FILE)\n",
    "idalia_drifter_df = read_drifter_data(IDALIA_DRIFTER_FILE)\n",
    "lee_drifter_df = read_drifter_data(LEE_DRIFTER_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATOMIC SWIFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: read in ATOMIC SWIFTs and convert to dataframe in transform\n",
    "def read_swift_directory(path: str) -> dict:\n",
    "    \"\"\"Helper function to read a directory of SWIFT .nc files.\n",
    "\n",
    "    Returns a dictionary of xarray Datasets. Requires 'SWIFT[id]' to be\n",
    "    in the individual filenames.\n",
    "    \"\"\"\n",
    "    filenames = glob.glob(path + \"/*.nc\")\n",
    "    swifts = {}\n",
    "    for file in filenames:\n",
    "        swift_id = re.search('SWIFT[0-9]{2}', file).group()\n",
    "        swifts[swift_id] = xr.open_dataset(file)\n",
    "\n",
    "    return swifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATOMIC_SWIFT_DIR = config['dirs']['atomic_swift']\n",
    "atomic_swifts = read_swift_directory(ATOMIC_SWIFT_DIR)\n",
    "\n",
    "#TODO: combine all SWIFTs into single ds or df\n",
    "\n",
    "\n",
    "all_atomic_swift_ds = []\n",
    "# for swift_id in atomic_swifts.keys():\n",
    "for swift_id in ['SWIFT16']:\n",
    "    atomic_swift_ds = atomic_swifts[swift_id]\n",
    "    atomic_swift_ds = atomic_swift_ds.expand_dims(swift_id=[swift_id])\n",
    "    all_atomic_swift_ds.append(atomic_swift_ds)\n",
    "\n",
    "\n",
    "atomic_swift_ds = xr.concat(all_atomic_swift_ds, dim='swift_id', coords='minimal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NHC_DIR = config['dirs']['nhc']\n",
    "\n",
    "def construct_nhc_path(storm_id: str, feature: str) -> str:\n",
    "    folder = f'{storm_id.lower()}_best_track'\n",
    "    filename = f'{storm_id.upper()}_{feature}.shp'\n",
    "    return os.path.join(NHC_DIR, folder, filename)\n",
    "\n",
    "def read_nhc_best_track(storm_id):\n",
    "    pts = geo.read_shp_file(construct_nhc_path(storm_id, 'pts'), index_by_datetime=True)\n",
    "    pts = geo.best_track_pts_to_intensity(pts)\n",
    "    lin = geo.read_shp_file(construct_nhc_path(storm_id, 'lin'))\n",
    "    windswath = geo.read_shp_file(construct_nhc_path(storm_id, 'windswath'))\n",
    "    # radii = geo.read_shp_file(construct_nhc_path(storm_id, 'radii'))\n",
    "    return pts, lin, windswath #, radii\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "earl_best_track = read_nhc_best_track(storm_id=earl_ds.attrs['storm_id'])\n",
    "fiona_best_track = read_nhc_best_track(storm_id=fiona_ds.attrs['storm_id'])\n",
    "ian_best_track = read_nhc_best_track(storm_id=ian_ds.attrs['storm_id'])\n",
    "julia_best_track = read_nhc_best_track(storm_id=julia_ds.attrs['storm_id'])\n",
    "idalia_best_track = read_nhc_best_track(storm_id=idalia_ds.attrs['storm_id'])\n",
    "lee_best_track = read_nhc_best_track(storm_id=lee_ds.attrs['storm_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBTrACS\n",
    "\n",
    "https://www.ncei.noaa.gov/products/international-best-track-archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'ibtracs_df' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "IBTRACS_BASE_URL = ('https://www.ncei.noaa.gov/data/international-best-'\n",
    "                    'track-archive-for-climate-stewardship-ibtracs/'\n",
    "                    'v04r00/access/csv/')\n",
    "IBTRACS_BASE_CSV = f'ibtracs.last3years.list.v04r00.csv'\n",
    "IBTRACS_PATH = config['dirs']['ibtracs']\n",
    "# ibtracs_df = pd.read_csv(IBTRACS_BASE_URL + IBTRACS_BASE_CSV, low_memory=False)\n",
    "ibtracs_df = pd.read_csv(IBTRACS_PATH, low_memory=False)\n",
    "\n",
    "idalia_ibtracs_df = (ibtracs_df\n",
    "    .query('NAME == \"IDALIA\"')\n",
    "    .assign(ISO_TIME = lambda df: pd.to_datetime(df['ISO_TIME']))\n",
    "    .set_index('ISO_TIME', drop=True)\n",
    "    .assign(LAT = lambda df: df['LAT'].astype(np.float64))\n",
    "    .assign(LON = lambda df: df['LON'].astype(np.float64))\n",
    ")\n",
    "\n",
    "ian_ibtracs_df = (ibtracs_df\n",
    "    .query('NAME == \"IAN\"')\n",
    "    .assign(ISO_TIME = lambda df: pd.to_datetime(df['ISO_TIME']))\n",
    "    .set_index('ISO_TIME', drop=True)\n",
    "    .assign(LAT = lambda df: df['LAT'].astype(np.float64))\n",
    "    .assign(LON = lambda df: df['LON'].astype(np.float64))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%store earl_ds\n",
    "%store fiona_ds\n",
    "%store ian_ds\n",
    "%store julia_ds\n",
    "%store idalia_ds\n",
    "%store lee_ds\n",
    "%store atomic_ds\n",
    "\n",
    "%store earl_drifter_df\n",
    "%store fiona_drifter_df\n",
    "%store ian_drifter_df\n",
    "%store idalia_drifter_df\n",
    "%store lee_drifter_df\n",
    "%store atomic_swift_ds\n",
    "\n",
    "%store earl_best_track\n",
    "%store fiona_best_track\n",
    "%store ian_best_track\n",
    "# %store julia_best_track\n",
    "%store idalia_best_track\n",
    "%store lee_best_track\n",
    "\n",
    "%store idalia_ibtracs_df\n",
    "%store ian_ibtracs_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyWSRA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
